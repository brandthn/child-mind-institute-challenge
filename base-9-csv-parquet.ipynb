{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Competition: Child Mind Institute - Problematic Internet Usage\n",
    "\n",
    "- ## Prédiction de l'utilisation problématique d'Internet chez les enfants\n",
    "- ## Utilisation de modèles deep learning pour analyser l'activité physique\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN_KAGGLE: False\n"
     ]
    }
   ],
   "source": [
    "# Vérification environnement Kaggle\n",
    "IN_KAGGLE = 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n",
    "\n",
    "# Configuration du chemin des données\n",
    "if IN_KAGGLE:\n",
    "    DATA_PATH = '/kaggle/input/child-mind-institute-problematic-internet-use'\n",
    "else:\n",
    "    DATA_PATH = '.'\n",
    "print(f'IN_KAGGLE: {IN_KAGGLE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration de TensorBoard pour Kaggle\n",
    "def setup_tensorboard():\n",
    "    \"\"\"\n",
    "    Configure TensorBoard en fonction de l'environnement\n",
    "    \"\"\"\n",
    "    if IN_KAGGLE:\n",
    "        log_dir = '/kaggle/working/logs'\n",
    "    else:\n",
    "        log_dir = 'logs'\n",
    "    \n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    return log_dir\n",
    "\n",
    "# Configuration des seeds pour reproductibilité\n",
    "def set_seeds(seed=25):\n",
    "    \"\"\"\n",
    "    Configure les seeds pour la reproductibilité\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.keras.utils.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_accelerometer_data(data_path, is_train=True):\n",
    "    \"\"\"\n",
    "    Charge les données d'accéléromètre en ignorant les fichiers système\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    def process_file(filename, dirname):\n",
    "        \"\"\"Traite un fichier parquet individuel\"\"\"\n",
    "        try:\n",
    "            df = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\n",
    "            \n",
    "            # Calcul des statistiques par colonnes\n",
    "            stats = []\n",
    "            for col in ['X', 'Y', 'Z', 'enmo', 'anglez']:\n",
    "                desc = df[col].describe()\n",
    "                stats.extend([desc['mean'], desc['std'], desc['max']])\n",
    "            \n",
    "            stats.extend([\n",
    "                df['non-wear_flag'].mean(),\n",
    "                df['light'].mean(),\n",
    "                df['weekday'].mean()\n",
    "            ])\n",
    "            \n",
    "            return stats, filename.split('=')[1]\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors du traitement du fichier {filename}: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    # chemin du dossier\n",
    "    dirname = os.path.join(data_path, 'series_train.parquet' if is_train else 'series_test.parquet')\n",
    "    \n",
    "    # Liste des IDs (sous-dossiers) en filtrant autres fichiers cachés (pa ex .DS_Store sur Mac OS)\n",
    "    ids = [d for d in os.listdir(dirname) \n",
    "           if not d.startswith('.') and  # Ignore les fichiers cachés\n",
    "           os.path.isdir(os.path.join(dirname, d)) and  # Vérifie que c'est un dossier\n",
    "           d.startswith('id=')]  # Vérifie que c'est un dossier d'ID\n",
    "    \n",
    "    print(f\"Traitement de {len(ids)} fichiers valides...\")\n",
    "    # Traitement parallèle\n",
    "    valid_results = []\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = list(tqdm(\n",
    "            executor.map(lambda fname: process_file(fname, dirname), ids),\n",
    "            total=len(ids)\n",
    "        ))\n",
    "        valid_results = [r for r in futures if r is not None]\n",
    "    \n",
    "    if not valid_results:\n",
    "        raise ValueError(\"Aucune donnée n'a pu être extraite\")\n",
    "    \n",
    "    # Séparation statistiques et des IDs\n",
    "    stats, indexes = zip(*valid_results)\n",
    "    \n",
    "    # noms de colonnes (accelerometre)\n",
    "    col_names = []\n",
    "    for col in ['X', 'Y', 'Z', 'enmo', 'anglez']:\n",
    "        col_names.extend([f\"accel_{col}_mean\", f\"accel_{col}_std\", f\"accel_{col}_max\"])\n",
    "    col_names.extend(['accel_nonwear_mean', 'accel_light_mean', 'accel_weekday_mean'])\n",
    "    \n",
    "    # DataFrame accel chargé\n",
    "    accel_features = pd.DataFrame(stats, columns=col_names)\n",
    "    accel_features['id'] = indexes\n",
    "    \n",
    "    return accel_features\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Charge données .csv et .parquet\n",
    "    \"\"\"\n",
    "    print(f\"Chargement des données depuis {DATA_PATH}\")\n",
    "    \n",
    "    # Chargement données tabulaires\n",
    "    train_df = pd.read_csv(os.path.join(DATA_PATH, 'train.csv'))\n",
    "    test_df = pd.read_csv(os.path.join(DATA_PATH, 'test.csv'))\n",
    "    \n",
    "    print(f\"Dimensions des données tabulaires:\")\n",
    "    print(f\"Train: {train_df.shape}\")\n",
    "    print(f\"Test: {test_df.shape}\")\n",
    "    \n",
    "    # Chargement données d'accéléromètre\n",
    "    print(\"Chargement des données d'accéléromètre...\")\n",
    "    try:\n",
    "        accel_train = load_accelerometer_data(DATA_PATH, is_train=True)\n",
    "        accel_test = load_accelerometer_data(DATA_PATH, is_train=False)\n",
    "        \n",
    "        print(f\"Dimensions des données d'accéléromètre:\")\n",
    "        print(f\"Train: {accel_train.shape}\")\n",
    "        print(f\"Test: {accel_test.shape}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du chargement des données d'accéléromètre: {str(e)}\")\n",
    "        raise\n",
    "    \n",
    "    return train_df, test_df, accel_train, accel_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_missing_indicators(df):\n",
    "    \"\"\"\n",
    "    Crée des indicateurs de val manquantes pour chaque colonne\n",
    "    \"\"\"\n",
    "    missing_indicators = pd.DataFrame()\n",
    "    for col in df.select_dtypes(include=['float64', 'int64']).columns:\n",
    "        if df[col].isnull().any():\n",
    "            missing_indicators[f'{col}_missing'] = df[col].isnull().astype(int)\n",
    "    return missing_indicators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-utilisé pour le moment\n",
    "def impute_missing_accel_features(features_df, all_ids, accel_features):\n",
    "    \"\"\"\n",
    "    Impute les valeurs manquantes pour les IDs sans données d'accéléromètre\n",
    "    \"\"\"\n",
    "    # Créer un DataFrame avec tous les IDs\n",
    "    complete_features = pd.DataFrame(index=all_ids)\n",
    "    \n",
    "    # Ajouter un flag pour indiquer la présence de données d'accéléromètre\n",
    "    complete_features['has_accel_data'] = complete_features.index.isin(accel_features.index).astype(int)\n",
    "    \n",
    "    # Fusionner avec les features existantes\n",
    "    complete_features = complete_features.join(accel_features)\n",
    "    \n",
    "    # Imputation des valeurs manquantes pour chaque type de feature\n",
    "    for col in complete_features.columns:\n",
    "        if col == 'has_accel_data':\n",
    "            continue\n",
    "            \n",
    "        if 'non-wear_flag' in col:\n",
    "            complete_features[col].fillna(1, inplace=True)\n",
    "        elif any(x in col for x in ['X_', 'Y_', 'Z_', 'anglez_', 'enmo_', 'light_']):\n",
    "            complete_features[col].fillna(0, inplace=True)\n",
    "        else:  # weekday et time_of_day\n",
    "            complete_features[col].fillna(complete_features[col].mean(), inplace=True)\n",
    "    \n",
    "    return complete_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_categorical_seasons(df):\n",
    "    \"\"\"\n",
    "    Convertit les colonnes de saisons en variables dummy\n",
    "    \"\"\"\n",
    "    season_columns = [col for col in df.columns if col.endswith('Season')]\n",
    "    for col in season_columns:\n",
    "        if col in df.columns:\n",
    "            season_dummies = pd.get_dummies(df[col], prefix=col, dummy_na=True)\n",
    "            df = pd.concat([df, season_dummies], axis=1)\n",
    "            df = df.drop(col, axis=1)\n",
    "    return df\n",
    "\n",
    "def preprocess_tabular_data(train_df, test_df, target_col='sii'):\n",
    "    \"\"\"\n",
    "    Prétraite les données tabulaires\n",
    "    \"\"\"\n",
    "    # 1. Suppression des lignes sans target dans train_df\n",
    "    train_df = train_df.dropna(subset=[target_col])\n",
    "    \n",
    "    # 2. Identification des colonnes communes car on veut matcher les variables dans les 2 datasets\n",
    "    common_columns = list(set(test_df.columns) & set(train_df.columns))\n",
    "    train_df = train_df[common_columns + [target_col]]\n",
    "    \n",
    "    # 3. Séparation des colonnes numériques et catégorielles car traitement différent pour chacun\n",
    "    numeric_columns = train_df.select_dtypes(include=['float64', 'int64', 'float32', 'int32']).columns\n",
    "    numeric_columns = [col for col in numeric_columns if col != target_col]\n",
    "    \n",
    "    # 4. Indicateurs de valeurs manquantes\n",
    "    train_missing = create_missing_indicators(train_df)\n",
    "    test_missing = create_missing_indicators(test_df)\n",
    "    \n",
    "    # 5. Traitement variables catégorielles (saisons)\n",
    "    train_df = process_categorical_seasons(train_df)\n",
    "    test_df = process_categorical_seasons(test_df)\n",
    "    \n",
    "    # 6. Imputation KNN pour variables numériques\n",
    "    imputer = KNNImputer(n_neighbors=5)\n",
    "    train_numeric = train_df[numeric_columns].copy()\n",
    "    test_numeric = test_df[numeric_columns].copy()\n",
    "    \n",
    "    train_numeric_imputed = pd.DataFrame(\n",
    "        imputer.fit_transform(train_numeric),\n",
    "        columns=train_numeric.columns,\n",
    "        index=train_numeric.index\n",
    "    )\n",
    "    test_numeric_imputed = pd.DataFrame(\n",
    "        imputer.transform(test_numeric),\n",
    "        columns=test_numeric.columns,\n",
    "        index=test_numeric.index\n",
    "    )\n",
    "    \n",
    "    # 7. Standardisation des variables numériques, on pourrait également utiliser RobustScaler puis MinMaxScaler\n",
    "    scaler = StandardScaler()\n",
    "    train_numeric_scaled = pd.DataFrame(\n",
    "        scaler.fit_transform(train_numeric_imputed),\n",
    "        columns=train_numeric_imputed.columns,\n",
    "        index=train_numeric_imputed.index\n",
    "    )\n",
    "    test_numeric_scaled = pd.DataFrame(\n",
    "        scaler.transform(test_numeric_imputed),\n",
    "        columns=test_numeric_imputed.columns,\n",
    "        index=test_numeric_imputed.index\n",
    "    )\n",
    "    \n",
    "    # 8. dataset au complet, on concatène les variables numériques, catégorielles et les indicateurs de valeurs manquantes\n",
    "    train_processed = pd.concat([\n",
    "        train_numeric_scaled,\n",
    "        train_df.select_dtypes(include=['object']),\n",
    "        train_missing\n",
    "    ], axis=1)\n",
    "    \n",
    "    test_processed = pd.concat([\n",
    "        test_numeric_scaled,\n",
    "        test_df.select_dtypes(include=['object']),\n",
    "        test_missing\n",
    "    ], axis=1)\n",
    "    \n",
    "    # 9. Récupération des targets\n",
    "    y = train_df[target_col].values\n",
    "    \n",
    "    return train_processed, test_processed, y, scaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "def extract_accel_features(df):\n",
    "    \"\"\"\n",
    "    Extrait features statistiques des données d'accéléromètre : accel_ moyenne, std, max, min, médiane\n",
    "    \"\"\"\n",
    "    features = df.groupby('id').agg({\n",
    "        'X': ['mean', 'std', 'max', 'min', 'median'],\n",
    "        'Y': ['mean', 'std', 'max', 'min', 'median'],\n",
    "        'Z': ['mean', 'std', 'max', 'min', 'median'],\n",
    "        'enmo': ['mean', 'std', 'max', 'min', 'median'],\n",
    "        'anglez': ['mean', 'std', 'max', 'min', 'median'],\n",
    "        'non-wear_flag': ['mean', 'sum'],\n",
    "        'light': ['mean', 'std', 'max', 'min'],\n",
    "        'weekday': ['mean', 'std'],\n",
    "        'time_of_day': ['mean', 'std']\n",
    "    })\n",
    "    \n",
    "    # Aplatir les noms de colonnes multi-index\n",
    "    features.columns = [f\"accel_{col[0]}_{col[1]}\" for col in features.columns]\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_categorical_seasons(df):\n",
    "    \"\"\"\n",
    "    Convertit les colonnes de saisons en variables dummy\n",
    "    \"\"\"\n",
    "    season_columns = [col for col in df.columns if col.endswith('Season')]\n",
    "    for col in season_columns:\n",
    "        if col in df.columns:\n",
    "            season_dummies = pd.get_dummies(df[col], prefix=col, dummy_na=True)\n",
    "            df = pd.concat([df, season_dummies], axis=1)\n",
    "            df = df.drop(col, axis=1)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_model(train_df, test_df, accel_train, accel_test, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Prépare les données pour l'entraînement du modèle avec les features d'accéléromètre pré-calculées\n",
    "    \"\"\"\n",
    "    # 1. Suppression des lignes sans target\n",
    "    train_df = train_df.dropna(subset=['sii'])\n",
    "    \n",
    "    # 2. Fusion des données tabulaires & d'accéléromètre\n",
    "    # Les features sont déjà calculées, on fait directement le merge\n",
    "    train_df = train_df.merge(accel_train, on='id', how='left')\n",
    "    test_df = test_df.merge(accel_test, on='id', how='left')\n",
    "    \n",
    "    # 3. Identification colonnes communes\n",
    "    common_columns = list(set(test_df.columns) & set(train_df.columns))\n",
    "    y = train_df['sii'].values\n",
    "    train_df = train_df[common_columns]\n",
    "    \n",
    "    # 4. Indicateurs valeurs manquantes\n",
    "    train_missing = create_missing_indicators(train_df)\n",
    "    test_missing = create_missing_indicators(test_df)\n",
    "    \n",
    "    # 5. Traitement variables catégorielles (saisons)\n",
    "    train_df = process_categorical_seasons(train_df)\n",
    "    test_df = process_categorical_seasons(test_df)\n",
    "    \n",
    "    # 6. Identification des colonnes numériques\n",
    "    numeric_columns = train_df.select_dtypes(include=['float64', 'int64', 'float32', 'int32']).columns\n",
    "    \n",
    "    # 7. Imputation KNN pour variables numériques\n",
    "    imputer = KNNImputer(n_neighbors=5)\n",
    "    \n",
    "    train_numeric = pd.DataFrame(\n",
    "        imputer.fit_transform(train_df[numeric_columns]),\n",
    "        columns=numeric_columns,\n",
    "        index=train_df.index\n",
    "    )\n",
    "    \n",
    "    test_numeric = pd.DataFrame(\n",
    "        imputer.transform(test_df[numeric_columns]),\n",
    "        columns=numeric_columns,\n",
    "        index=test_df.index\n",
    "    )\n",
    "    \n",
    "    # 8. Standardisation\n",
    "    scaler = StandardScaler()\n",
    "    train_numeric_scaled = pd.DataFrame(\n",
    "        scaler.fit_transform(train_numeric),\n",
    "        columns=train_numeric.columns,\n",
    "        index=train_numeric.index\n",
    "    )\n",
    "    \n",
    "    test_numeric_scaled = pd.DataFrame(\n",
    "        scaler.transform(test_numeric),\n",
    "        columns=test_numeric.columns,\n",
    "        index=test_numeric.index\n",
    "    )\n",
    "    \n",
    "    # 9. Reconstruction des datasets finaux\n",
    "    train_processed = pd.concat([\n",
    "        train_numeric_scaled,\n",
    "        train_df.select_dtypes(include=['object']),\n",
    "        train_missing\n",
    "    ], axis=1)\n",
    "    \n",
    "    test_processed = pd.concat([\n",
    "        test_numeric_scaled,\n",
    "        test_df.select_dtypes(include=['object']),\n",
    "        test_missing\n",
    "    ], axis=1)\n",
    "    \n",
    "    # 10. Split avec stratification sur la target\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        train_processed, y,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    return X_train, X_val, test_processed, y_train, y_val, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement  données\n",
    "\n",
    "#data_path = '/kaggle/input/child-mind-institute-problematic-internet-use'\n",
    "#train_df = pd.read_csv(f'{data_path}/train.csv')\n",
    "#test_df = pd.read_csv(f'{data_path}/test.csv')\n",
    "#accel_train = pd.read_parquet(f'{data_path}/series_train.parquet')\n",
    "#accel_test = pd.read_parquet(f'{data_path}/series_test.parquet')\n",
    "\n",
    "#train_df = pd.read_csv('train.csv')\n",
    "#test_df = pd.read_csv('test.csv')\n",
    "#accel_train = pd.read_parquet('series_train.parquet')\n",
    "#accel_test = pd.read_parquet('series_test.parquet')\n",
    "\n",
    "# Preprocessing des données\n",
    "X_train, X_val, X_test, y_train, y_val, scaler = prepare_data_for_model(\n",
    "    train_df, test_df, accel_train, accel_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement des données depuis .\n",
      "Dimensions des données tabulaires:\n",
      "Train: (3960, 82)\n",
      "Test: (20, 59)\n",
      "Chargement des données d'accéléromètre...\n",
      "Traitement de 996 fichiers valides...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 996/996 [00:08<00:00, 115.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement de 2 fichiers valides...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 40.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions des données d'accéléromètre:\n",
      "Train: (996, 19)\n",
      "Test: (2, 19)\n"
     ]
    }
   ],
   "source": [
    "# Configuration initiale (inchangée)\n",
    "IN_KAGGLE = 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n",
    "DATA_PATH = '/kaggle/input/child-mind-institute-problematic-internet-use' if IN_KAGGLE else '.'\n",
    "\n",
    "# Chargement des données avec la nouvelle approche\n",
    "train_df, test_df, accel_train, accel_test = load_data()\n",
    "\n",
    "# Le reste de votre pipeline reste inchangé\n",
    "X_train, X_val, X_test, y_train, y_val, scaler = prepare_data_for_model(\n",
    "    train_df, test_df, accel_train, accel_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(996,)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accel_train.id.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total d'IDs dans test_df: 20\n",
      "Nombre d'IDs dans accel_test: 2\n",
      "IDs dans test_df: ['00008ff9' '000fd460' '00105258' '00115b9f' '0016bb22' '001f3379'\n",
      " '0038ba98' '0068a485' '0069fbed' '0083e397' '0087dd65' '00abe655'\n",
      " '00ae59c9' '00af6387' '00bd4359' '00c0cd71' '00d56d4b' '00d9913d'\n",
      " '00e6167c' '00ebc35d']\n",
      "IDs dans accel_test: ['001f3379' '00115b9f']\n"
     ]
    }
   ],
   "source": [
    "print(\"Nombre total d'IDs dans test_df:\", test_df.id.nunique())\n",
    "print(\"Nombre d'IDs dans accel_test:\", accel_test.id.nunique())\n",
    "print(\"IDs dans test_df:\", test_df.id.unique())\n",
    "print(\"IDs dans accel_test:\", accel_test.id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_head = X_train.head()\n",
    "X_val_head = X_val.head()\n",
    "X_test_head = X_test.head()\n",
    "y_train_head = y_train[:5]\n",
    "y_val_head = y_val[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(    FGC-FGC_GSD  FGC-FGC_SRR  PAQ_C-PAQ_C_Total  FGC-FGC_PU_Zone  BIA-BIA_FMI  \\\n",
       " 0     -1.020359    -1.068121           0.137091        -0.888445    -0.002662   \n",
       " 1      0.017831     0.662279          -0.396029        -0.888445    -0.296235   \n",
       " 2     -0.677891     0.316199          -0.672341         1.539911     0.059101   \n",
       " 3      0.921192    -0.722041          -0.215613        -0.888445     0.181270   \n",
       " 4      1.158492    -1.275769          -0.854057        -0.402773     1.154649   \n",
       " 5     -0.246435     0.662279           2.480869        -0.888445     1.653885   \n",
       " 6     -0.367782     0.662279           1.765708        -0.888445     0.299128   \n",
       " 7     -1.163278    -3.144601          -2.135171        -0.888445     0.027893   \n",
       " 8      1.511746    -0.549001          -0.613828        -0.402773     0.850448   \n",
       " 9      1.465904    -0.860473          -0.532235         0.082898     1.136931   \n",
       " 10    -0.472949     0.108551           0.585692        -0.402773     0.132090   \n",
       " 11    -0.205986    -0.756649          -2.411483         0.082898     0.535546   \n",
       " 12    -0.179020     0.143159           0.709220         1.539911     0.371936   \n",
       " 13     0.252435    -0.029881          -2.216439        -0.888445     0.068613   \n",
       " 14    -0.054977    -0.825865           0.656883         0.568569     0.772035   \n",
       " 15     0.228166     2.046599          -0.116466        -0.888445    -1.119231   \n",
       " 16    -0.100819     0.316199          -0.558565        -0.888445    -0.120490   \n",
       " 17    -0.607779    -3.144601          -0.639834        -0.888445     0.285654   \n",
       " 18    -0.518792    -1.760281          -0.613828        -0.888445     0.016802   \n",
       " 19    -0.629352    -0.375961           0.127339        -0.402773     0.285330   \n",
       " \n",
       "     accel_anglez_max  accel_Y_std  BIA-BIA_Fat  Basic_Demos-Sex  \\\n",
       " 0           0.264262     0.544735    -0.018042        -0.757178   \n",
       " 1          -0.128817    -0.251204    -0.048785        -0.757178   \n",
       " 2           0.096962    -0.184413     0.017036         1.320694   \n",
       " 3           0.395813     0.835363     0.038312        -0.757178   \n",
       " 4          -0.283886    -0.350138     0.274175         1.320694   \n",
       " 5           0.267563    -1.847426     0.326501         1.320694   \n",
       " 6           0.067344     0.210278     0.053140        -0.757178   \n",
       " 7          -0.062148    -0.952505     0.023202         1.320694   \n",
       " 8          -1.390127     0.445762     0.226144        -0.757178   \n",
       " 9           0.011198     0.159476     0.277491         1.320694   \n",
       " 10          0.228132    -0.182538     0.043489         1.320694   \n",
       " 11          0.092758    -0.222738     0.120809        -0.757178   \n",
       " 12         -0.132990    -1.005368     0.078734        -0.757178   \n",
       " 13          0.053337    -0.306362     0.019057        -0.757178   \n",
       " 14         -0.006594     1.112628     0.199364        -0.757178   \n",
       " 15          0.284359     0.821656    -0.155706        -0.757178   \n",
       " 16          0.263109    -1.050910    -0.032816         1.320694   \n",
       " 17          0.013027     0.215988     0.051718         1.320694   \n",
       " 18          0.263758    -0.178946    -0.007001        -0.757178   \n",
       " 19          0.300204     0.911408     0.059527        -0.757178   \n",
       " \n",
       "     Physical-Diastolic_BP  ...  accel_Z_mean_missing  accel_Z_std_missing  \\\n",
       " 0               -0.488477  ...                     1                    1   \n",
       " 1                0.387483  ...                     1                    1   \n",
       " 2               -0.367655  ...                     1                    1   \n",
       " 3               -0.745224  ...                     0                    0   \n",
       " 4                0.342175  ...                     1                    1   \n",
       " 5               -0.745224  ...                     0                    0   \n",
       " 6                4.012145  ...                     1                    1   \n",
       " 7                0.085428  ...                     1                    1   \n",
       " 8               -0.005189  ...                     1                    1   \n",
       " 9                0.221353  ...                     1                    1   \n",
       " 10               0.357277  ...                     1                    1   \n",
       " 11               0.568716  ...                     1                    1   \n",
       " 12              -0.518683  ...                     1                    1   \n",
       " 13              -0.971765  ...                     1                    1   \n",
       " 14              -0.382758  ...                     1                    1   \n",
       " 15               0.765052  ...                     1                    1   \n",
       " 16              -0.669710  ...                     1                    1   \n",
       " 17               0.674435  ...                     1                    1   \n",
       " 18              -0.669710  ...                     1                    1   \n",
       " 19               0.009914  ...                     1                    1   \n",
       " \n",
       "     accel_Z_max_missing  accel_enmo_mean_missing  accel_enmo_std_missing  \\\n",
       " 0                     1                        1                       1   \n",
       " 1                     1                        1                       1   \n",
       " 2                     1                        1                       1   \n",
       " 3                     0                        0                       0   \n",
       " 4                     1                        1                       1   \n",
       " 5                     0                        0                       0   \n",
       " 6                     1                        1                       1   \n",
       " 7                     1                        1                       1   \n",
       " 8                     1                        1                       1   \n",
       " 9                     1                        1                       1   \n",
       " 10                    1                        1                       1   \n",
       " 11                    1                        1                       1   \n",
       " 12                    1                        1                       1   \n",
       " 13                    1                        1                       1   \n",
       " 14                    1                        1                       1   \n",
       " 15                    1                        1                       1   \n",
       " 16                    1                        1                       1   \n",
       " 17                    1                        1                       1   \n",
       " 18                    1                        1                       1   \n",
       " 19                    1                        1                       1   \n",
       " \n",
       "     accel_enmo_max_missing  accel_anglez_mean_missing  \\\n",
       " 0                        1                          1   \n",
       " 1                        1                          1   \n",
       " 2                        1                          1   \n",
       " 3                        0                          0   \n",
       " 4                        1                          1   \n",
       " 5                        0                          0   \n",
       " 6                        1                          1   \n",
       " 7                        1                          1   \n",
       " 8                        1                          1   \n",
       " 9                        1                          1   \n",
       " 10                       1                          1   \n",
       " 11                       1                          1   \n",
       " 12                       1                          1   \n",
       " 13                       1                          1   \n",
       " 14                       1                          1   \n",
       " 15                       1                          1   \n",
       " 16                       1                          1   \n",
       " 17                       1                          1   \n",
       " 18                       1                          1   \n",
       " 19                       1                          1   \n",
       " \n",
       "     accel_anglez_std_missing  accel_anglez_max_missing  \\\n",
       " 0                          1                         1   \n",
       " 1                          1                         1   \n",
       " 2                          1                         1   \n",
       " 3                          0                         0   \n",
       " 4                          1                         1   \n",
       " 5                          0                         0   \n",
       " 6                          1                         1   \n",
       " 7                          1                         1   \n",
       " 8                          1                         1   \n",
       " 9                          1                         1   \n",
       " 10                         1                         1   \n",
       " 11                         1                         1   \n",
       " 12                         1                         1   \n",
       " 13                         1                         1   \n",
       " 14                         1                         1   \n",
       " 15                         1                         1   \n",
       " 16                         1                         1   \n",
       " 17                         1                         1   \n",
       " 18                         1                         1   \n",
       " 19                         1                         1   \n",
       " \n",
       "     accel_weekday_mean_missing  \n",
       " 0                            1  \n",
       " 1                            1  \n",
       " 2                            1  \n",
       " 3                            0  \n",
       " 4                            1  \n",
       " 5                            0  \n",
       " 6                            1  \n",
       " 7                            1  \n",
       " 8                            1  \n",
       " 9                            1  \n",
       " 10                           1  \n",
       " 11                           1  \n",
       " 12                           1  \n",
       " 13                           1  \n",
       " 14                           1  \n",
       " 15                           1  \n",
       " 16                           1  \n",
       " 17                           1  \n",
       " 18                           1  \n",
       " 19                           1  \n",
       " \n",
       " [20 rows x 129 columns],\n",
       " array([0., 0., 2., ..., 2., 1., 2.]),\n",
       " array([0., 1., 1., 1., 1., 2., 2., 0., 0., 2., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 1., 2., 0., 0., 2., 1., 0., 2., 0., 3., 0., 2., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 2., 2., 0., 2., 2., 0., 1., 0., 0., 0., 1.,\n",
       "        1., 1., 0., 0., 0., 1., 0., 0., 2., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
       "        1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 3., 1., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0.,\n",
       "        0., 0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 2.,\n",
       "        1., 0., 1., 0., 0., 0., 1., 2., 0., 3., 0., 1., 0., 2., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 1., 0., 2., 0., 1., 0., 0., 0., 1., 0., 0., 2., 0.,\n",
       "        1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 2., 2., 1., 1., 0., 0., 1.,\n",
       "        1., 0., 0., 1., 0., 0., 0., 2., 0., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
       "        2., 1., 0., 1., 0., 0., 0., 0., 0., 2., 0., 0., 1., 1., 0., 0., 1.,\n",
       "        1., 0., 0., 0., 2., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 2., 1., 0., 1., 0., 0., 0., 2., 0., 0., 1., 2.,\n",
       "        1., 0., 0., 0., 1., 1., 0., 0., 2., 0., 0., 0., 0., 2., 1., 2., 0.,\n",
       "        2., 1., 0., 1., 0., 0., 0., 0., 1., 3., 0., 2., 0., 1., 1., 0., 0.,\n",
       "        1., 2., 2., 1., 0., 1., 2., 3., 0., 0., 0., 0., 0., 1., 2., 0., 0.,\n",
       "        1., 0., 1., 2., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1.,\n",
       "        0., 1., 1., 1., 0., 0., 0., 2., 2., 0., 0., 1., 0., 1., 2., 0., 0.,\n",
       "        1., 2., 1., 0., 0., 0., 2., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1.,\n",
       "        0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 2., 2., 1., 1., 1., 0., 0., 0., 2., 0., 1., 1., 2.,\n",
       "        1., 0., 0., 0., 0., 2., 0., 0., 1., 2., 1., 2., 2., 0., 0., 2., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 2., 1., 2., 2., 1., 0., 0.,\n",
       "        0., 2., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 2.,\n",
       "        1., 0., 2., 1., 2., 1., 2., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 2., 1., 1., 2., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
       "        0., 2., 0., 1., 1., 2., 1., 3., 1., 0., 0., 0., 0., 0., 2., 1., 2.,\n",
       "        0., 0., 0., 2., 2., 0., 2., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
       "        0., 0., 1., 0., 0., 2., 0., 2., 0., 1., 0., 0., 0., 2., 0., 0., 2.,\n",
       "        0., 0., 0., 1., 0., 0., 2., 1., 2., 0., 1., 2., 1., 2., 0., 0., 1.,\n",
       "        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 3., 1., 0., 1., 1., 0.,\n",
       "        0., 0., 1., 0.]))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()\n",
    "X_val.head()\n",
    "X_test, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_training(X_train, X_val, X_test, y_train, y_val):\n",
    "    \"\"\"\n",
    "    Prépare les données pour l'entraînement \n",
    "    \"\"\"\n",
    "    # Suppression des colonnes non numériques\n",
    "    if isinstance(X_train, pd.DataFrame):\n",
    "        # Identification des colonnes numériques\n",
    "        numeric_cols = X_train.select_dtypes(include=['float64', 'float32', 'int64', 'int32']).columns\n",
    "        X_train = X_train[numeric_cols]\n",
    "        X_val = X_val[numeric_cols]\n",
    "        if X_test is not None:\n",
    "            X_test = X_test[numeric_cols]\n",
    "    \n",
    "    # Conversion en array numpy si ce n'est pas déjà fait\n",
    "    if isinstance(X_train, pd.DataFrame):\n",
    "        X_train = X_train.astype('float32').to_numpy()\n",
    "    if isinstance(X_val, pd.DataFrame):\n",
    "        X_val = X_val.astype('float32').to_numpy()\n",
    "    if X_test is not None and isinstance(X_test, pd.DataFrame):\n",
    "        X_test = X_test.astype('float32').to_numpy()\n",
    "        \n",
    "    # Conversion des labels en int32\n",
    "    y_train = y_train.astype('int32')\n",
    "    y_val = y_val.astype('int32')\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val\n",
    "\n",
    "def create_linear_model(input_shape):\n",
    "    \"\"\"\n",
    "    modèle linéaire\n",
    "    \"\"\"\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.Dense(4, activation='softmax', name='output')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def create_mlp_model(input_shape, dropout_rate=0.3):\n",
    "    \"\"\"\n",
    "    Crée un MLP\n",
    "    \"\"\"\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(4, activation='softmax', name='output')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def create_cnn_model(input_shape, dropout_rate=0.3):\n",
    "    \"\"\"\n",
    "    Crée un CNN 1D notamment pour données tabulaires\n",
    "    \"\"\"\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.Reshape((-1, 1)),\n",
    "        layers.Conv1D(32, 3, activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling1D(2),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Conv1D(64, 3, activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling1D(2),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Conv1D(128, 3, activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.GlobalAveragePooling1D(),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(4, activation='softmax', name='output')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def train_model(model, X_train, y_train, X_val, y_val, model_name, batch_size=32, epochs=100):\n",
    "    \"\"\"\n",
    "    Fonction d'entraînement modèle (avec TensorBoard et EarlyStopping)\n",
    "    \"\"\"\n",
    "    # Création du dossier pour les logs TensorBoard\n",
    "    log_dir = os.path.join(\"logs\", model_name + \"_\" + time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    tensorboard_callback = TensorBoard(\n",
    "        log_dir=log_dir,\n",
    "        histogram_freq=1,\n",
    "        write_graph=True,\n",
    "        write_images=True\n",
    "    )\n",
    "    \n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # résumé du modèle\n",
    "    model.summary()\n",
    "    \n",
    "    # Entraînement\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks=[tensorboard_callback, early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return history\n",
    "\n",
    "def evaluate_model(model, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Évaluation et métriques modèles\n",
    "    \"\"\"\n",
    "    val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "    y_pred = model.predict(X_val)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    \n",
    "    return {\n",
    "        'loss': val_loss,\n",
    "        'accuracy': val_acc,\n",
    "        'predictions': y_pred,\n",
    "        'pred_classes': y_pred_classes\n",
    "    }\n",
    "\n",
    "def train_all_models(X_train, X_val, y_train, y_val, input_shape):\n",
    "    \"\"\"\n",
    "    Entraine et retourne leurs performances\n",
    "    \"\"\"\n",
    "    X_train, X_val, _, y_train, y_val = prepare_data_for_training(\n",
    "        X_train, X_val, None, y_train, y_val\n",
    "    )\n",
    "    \n",
    "    input_shape = (X_train.shape[1],)\n",
    "    \n",
    "    models = {\n",
    "        'linear': create_linear_model(input_shape),\n",
    "        'mlp': create_mlp_model(input_shape),\n",
    "        'cnn': create_cnn_model(input_shape)\n",
    "    }\n",
    "    \n",
    "    histories = {}\n",
    "    evaluations = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nEntraînement du modèle {name}\")\n",
    "        history = train_model(model, X_train, y_train, X_val, y_val, name)\n",
    "        histories[name] = history\n",
    "        \n",
    "        # Évaluation du modèle\n",
    "        eval_results = evaluate_model(model, X_val, y_val)\n",
    "        evaluations[name] = eval_results\n",
    "        \n",
    "        print(f\"{name} - Validation Loss: {eval_results['loss']:.4f}, \"\n",
    "              f\"Validation Accuracy: {eval_results['accuracy']:.4f}\")\n",
    "    \n",
    "    # Détermination du meilleur modèle\n",
    "    best_model_name = min(evaluations.items(), key=lambda x: x[1]['loss'])[0]\n",
    "    print(f\"\\nMeilleur modèle (basé sur la validation loss): {best_model_name}\")\n",
    "    \n",
    "    return models, histories, best_model_name, evaluations\n",
    "\n",
    "def generate_submission(best_model, X_test, test_df, output_file='submission.csv'):\n",
    "    \"\"\"\n",
    "    Génère le fichier de soumission avec les prédictions\n",
    "    \"\"\"\n",
    "    # Garde copie de l'ID avant préparation\n",
    "    test_ids = test_df['id'].copy()\n",
    "    \n",
    "    # Sélection uniquement des colonnes numériques\n",
    "    numeric_cols = X_test.select_dtypes(include=['float64', 'float32', 'int64', 'int32']).columns\n",
    "    X_test_numeric = X_test[numeric_cols].copy()\n",
    "    \n",
    "    # Conversion array numpy\n",
    "    X_test_prepared = X_test_numeric.astype('float32').to_numpy()\n",
    "    \n",
    "    # Génération prédictions\n",
    "    predictions = best_model.predict(X_test_prepared, verbose=0)\n",
    "    predictions_classes = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # Création DataFrame de soumission\n",
    "    submission = pd.DataFrame({\n",
    "        'id': test_ids,\n",
    "        'sii': predictions_classes\n",
    "    })\n",
    "\n",
    "    if IN_KAGGLE:\n",
    "        output_file = f'/kaggle/working/{output_file}'\n",
    "    \n",
    "    submission.to_csv(output_file, index=False)\n",
    "    print(f\"Fichier de soumission généré : {output_file}\")\n",
    "    \n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entraînement du modèle linear\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " output (Dense)              (None, 4)                 516       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 516 (2.02 KB)\n",
      "Trainable params: 516 (2.02 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 1.3963 - accuracy: 0.4292 - val_loss: 1.1922 - val_accuracy: 0.5255\n",
      "Epoch 2/100\n",
      "69/69 [==============================] - 0s 670us/step - loss: 1.1635 - accuracy: 0.5210 - val_loss: 1.0848 - val_accuracy: 0.5347\n",
      "Epoch 3/100\n",
      "69/69 [==============================] - 0s 664us/step - loss: 1.0713 - accuracy: 0.5430 - val_loss: 1.0247 - val_accuracy: 0.5529\n",
      "Epoch 4/100\n",
      "69/69 [==============================] - 0s 703us/step - loss: 1.0114 - accuracy: 0.5626 - val_loss: 0.9850 - val_accuracy: 0.5730\n",
      "Epoch 5/100\n",
      "69/69 [==============================] - 0s 666us/step - loss: 0.9716 - accuracy: 0.5791 - val_loss: 0.9619 - val_accuracy: 0.5730\n",
      "Epoch 6/100\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.9416 - accuracy: 0.5878 - val_loss: 0.9428 - val_accuracy: 0.5821\n",
      "Epoch 7/100\n",
      "69/69 [==============================] - 0s 678us/step - loss: 0.9189 - accuracy: 0.5987 - val_loss: 0.9280 - val_accuracy: 0.5858\n",
      "Epoch 8/100\n",
      "69/69 [==============================] - 0s 672us/step - loss: 0.9013 - accuracy: 0.6001 - val_loss: 0.9239 - val_accuracy: 0.5894\n",
      "Epoch 9/100\n",
      "69/69 [==============================] - 0s 647us/step - loss: 0.8902 - accuracy: 0.6069 - val_loss: 0.9130 - val_accuracy: 0.5967\n",
      "Epoch 10/100\n",
      "69/69 [==============================] - 0s 688us/step - loss: 0.8816 - accuracy: 0.6124 - val_loss: 0.9057 - val_accuracy: 0.5985\n",
      "Epoch 11/100\n",
      "69/69 [==============================] - 0s 657us/step - loss: 0.8772 - accuracy: 0.6138 - val_loss: 0.9040 - val_accuracy: 0.6004\n",
      "Epoch 12/100\n",
      "69/69 [==============================] - 0s 658us/step - loss: 0.8703 - accuracy: 0.6143 - val_loss: 0.9005 - val_accuracy: 0.5949\n",
      "Epoch 13/100\n",
      "69/69 [==============================] - 0s 651us/step - loss: 0.8655 - accuracy: 0.6179 - val_loss: 0.8963 - val_accuracy: 0.6004\n",
      "Epoch 14/100\n",
      "69/69 [==============================] - 0s 652us/step - loss: 0.8608 - accuracy: 0.6202 - val_loss: 0.8983 - val_accuracy: 0.5985\n",
      "Epoch 15/100\n",
      "69/69 [==============================] - 0s 654us/step - loss: 0.8599 - accuracy: 0.6184 - val_loss: 0.8940 - val_accuracy: 0.6040\n",
      "Epoch 16/100\n",
      "69/69 [==============================] - 0s 666us/step - loss: 0.8572 - accuracy: 0.6193 - val_loss: 0.8925 - val_accuracy: 0.6040\n",
      "Epoch 17/100\n",
      "69/69 [==============================] - 0s 670us/step - loss: 0.8539 - accuracy: 0.6152 - val_loss: 0.8923 - val_accuracy: 0.5967\n",
      "Epoch 18/100\n",
      "69/69 [==============================] - 0s 816us/step - loss: 0.8525 - accuracy: 0.6138 - val_loss: 0.8941 - val_accuracy: 0.5985\n",
      "Epoch 19/100\n",
      "69/69 [==============================] - 0s 755us/step - loss: 0.8508 - accuracy: 0.6179 - val_loss: 0.8935 - val_accuracy: 0.6004\n",
      "Epoch 20/100\n",
      "69/69 [==============================] - 0s 730us/step - loss: 0.8473 - accuracy: 0.6239 - val_loss: 0.8931 - val_accuracy: 0.6004\n",
      "Epoch 21/100\n",
      "69/69 [==============================] - 0s 796us/step - loss: 0.8477 - accuracy: 0.6197 - val_loss: 0.8903 - val_accuracy: 0.5949\n",
      "Epoch 22/100\n",
      "69/69 [==============================] - 0s 762us/step - loss: 0.8458 - accuracy: 0.6202 - val_loss: 0.8913 - val_accuracy: 0.5912\n",
      "Epoch 23/100\n",
      "69/69 [==============================] - 0s 760us/step - loss: 0.8461 - accuracy: 0.6243 - val_loss: 0.8900 - val_accuracy: 0.6022\n",
      "Epoch 24/100\n",
      "69/69 [==============================] - 0s 731us/step - loss: 0.8436 - accuracy: 0.6188 - val_loss: 0.8893 - val_accuracy: 0.5985\n",
      "Epoch 25/100\n",
      "69/69 [==============================] - 0s 734us/step - loss: 0.8424 - accuracy: 0.6252 - val_loss: 0.8938 - val_accuracy: 0.5949\n",
      "Epoch 26/100\n",
      "69/69 [==============================] - 0s 718us/step - loss: 0.8417 - accuracy: 0.6271 - val_loss: 0.8918 - val_accuracy: 0.6004\n",
      "Epoch 27/100\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.8419 - accuracy: 0.6243 - val_loss: 0.8947 - val_accuracy: 0.5949\n",
      "Epoch 28/100\n",
      "69/69 [==============================] - 0s 679us/step - loss: 0.8408 - accuracy: 0.6243 - val_loss: 0.8888 - val_accuracy: 0.6040\n",
      "Epoch 29/100\n",
      "69/69 [==============================] - 0s 655us/step - loss: 0.8400 - accuracy: 0.6229 - val_loss: 0.8891 - val_accuracy: 0.6040\n",
      "Epoch 30/100\n",
      "69/69 [==============================] - 0s 656us/step - loss: 0.8398 - accuracy: 0.6280 - val_loss: 0.8896 - val_accuracy: 0.6058\n",
      "Epoch 31/100\n",
      "69/69 [==============================] - 0s 647us/step - loss: 0.8375 - accuracy: 0.6225 - val_loss: 0.8895 - val_accuracy: 0.6113\n",
      "Epoch 32/100\n",
      "69/69 [==============================] - 0s 649us/step - loss: 0.8387 - accuracy: 0.6257 - val_loss: 0.8861 - val_accuracy: 0.6077\n",
      "Epoch 33/100\n",
      "69/69 [==============================] - 0s 677us/step - loss: 0.8361 - accuracy: 0.6266 - val_loss: 0.8893 - val_accuracy: 0.6022\n",
      "Epoch 34/100\n",
      "69/69 [==============================] - 0s 696us/step - loss: 0.8372 - accuracy: 0.6275 - val_loss: 0.8883 - val_accuracy: 0.5985\n",
      "Epoch 35/100\n",
      "69/69 [==============================] - 0s 669us/step - loss: 0.8367 - accuracy: 0.6252 - val_loss: 0.8901 - val_accuracy: 0.6077\n",
      "Epoch 36/100\n",
      "69/69 [==============================] - 0s 649us/step - loss: 0.8364 - accuracy: 0.6252 - val_loss: 0.8909 - val_accuracy: 0.6058\n",
      "Epoch 37/100\n",
      "69/69 [==============================] - 0s 684us/step - loss: 0.8348 - accuracy: 0.6239 - val_loss: 0.8889 - val_accuracy: 0.6077\n",
      "Epoch 38/100\n",
      "69/69 [==============================] - 0s 665us/step - loss: 0.8343 - accuracy: 0.6271 - val_loss: 0.8905 - val_accuracy: 0.6131\n",
      "Epoch 39/100\n",
      "69/69 [==============================] - 0s 677us/step - loss: 0.8339 - accuracy: 0.6284 - val_loss: 0.8908 - val_accuracy: 0.6113\n",
      "Epoch 40/100\n",
      "69/69 [==============================] - 0s 864us/step - loss: 0.8365 - accuracy: 0.6275 - val_loss: 0.8898 - val_accuracy: 0.6022\n",
      "Epoch 41/100\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.8331 - accuracy: 0.6248 - val_loss: 0.8904 - val_accuracy: 0.6077\n",
      "Epoch 42/100\n",
      "69/69 [==============================] - 0s 764us/step - loss: 0.8346 - accuracy: 0.6248 - val_loss: 0.8871 - val_accuracy: 0.6077\n",
      "18/18 [==============================] - 0s 322us/step\n",
      "linear - Validation Loss: 0.8861, Validation Accuracy: 0.6077\n",
      "\n",
      "Entraînement du modèle mlp\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_16 (Dense)            (None, 256)               33024     \n",
      "                                                                 \n",
      " batch_normalization_28 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_28 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_29 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_29 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_30 (Ba  (None, 64)                256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_30 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 4)                 260       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 76228 (297.77 KB)\n",
      "Trainable params: 75332 (294.27 KB)\n",
      "Non-trainable params: 896 (3.50 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "69/69 [==============================] - 1s 3ms/step - loss: 1.7662 - accuracy: 0.3560 - val_loss: 1.0505 - val_accuracy: 0.5949\n",
      "Epoch 2/100\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 1.3242 - accuracy: 0.4703 - val_loss: 0.9874 - val_accuracy: 0.5985\n",
      "Epoch 3/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 1.1488 - accuracy: 0.5343 - val_loss: 0.9422 - val_accuracy: 0.6113\n",
      "Epoch 4/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 1.0382 - accuracy: 0.5658 - val_loss: 0.9481 - val_accuracy: 0.6004\n",
      "Epoch 5/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.9917 - accuracy: 0.5919 - val_loss: 0.9394 - val_accuracy: 0.6113\n",
      "Epoch 6/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.9560 - accuracy: 0.6156 - val_loss: 0.9235 - val_accuracy: 0.6004\n",
      "Epoch 7/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.9170 - accuracy: 0.6179 - val_loss: 0.9241 - val_accuracy: 0.5949\n",
      "Epoch 8/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.9030 - accuracy: 0.6197 - val_loss: 0.9173 - val_accuracy: 0.6004\n",
      "Epoch 9/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.8740 - accuracy: 0.6202 - val_loss: 0.9084 - val_accuracy: 0.6113\n",
      "Epoch 10/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.8572 - accuracy: 0.6312 - val_loss: 0.9112 - val_accuracy: 0.5967\n",
      "Epoch 11/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.8352 - accuracy: 0.6357 - val_loss: 0.9158 - val_accuracy: 0.6004\n",
      "Epoch 12/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.8075 - accuracy: 0.6609 - val_loss: 0.9238 - val_accuracy: 0.5949\n",
      "Epoch 13/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.8144 - accuracy: 0.6572 - val_loss: 0.9165 - val_accuracy: 0.5931\n",
      "Epoch 14/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.7851 - accuracy: 0.6645 - val_loss: 0.9299 - val_accuracy: 0.5894\n",
      "Epoch 15/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.7878 - accuracy: 0.6609 - val_loss: 0.9348 - val_accuracy: 0.6022\n",
      "Epoch 16/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.7686 - accuracy: 0.6696 - val_loss: 0.9370 - val_accuracy: 0.5894\n",
      "Epoch 17/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.7542 - accuracy: 0.6764 - val_loss: 0.9590 - val_accuracy: 0.5766\n",
      "Epoch 18/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.7414 - accuracy: 0.6878 - val_loss: 0.9429 - val_accuracy: 0.5949\n",
      "Epoch 19/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.7303 - accuracy: 0.6929 - val_loss: 0.9607 - val_accuracy: 0.5912\n",
      "18/18 [==============================] - 0s 568us/step\n",
      "mlp - Validation Loss: 0.9084, Validation Accuracy: 0.6113\n",
      "\n",
      "Entraînement du modèle cnn\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_4 (Reshape)         (None, 128, 1)            0         \n",
      "                                                                 \n",
      " conv1d_12 (Conv1D)          (None, 128, 32)           128       \n",
      "                                                                 \n",
      " batch_normalization_31 (Ba  (None, 128, 32)           128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " max_pooling1d_8 (MaxPoolin  (None, 64, 32)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout_31 (Dropout)        (None, 64, 32)            0         \n",
      "                                                                 \n",
      " conv1d_13 (Conv1D)          (None, 64, 64)            6208      \n",
      "                                                                 \n",
      " batch_normalization_32 (Ba  (None, 64, 64)            256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " max_pooling1d_9 (MaxPoolin  (None, 32, 64)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout_32 (Dropout)        (None, 32, 64)            0         \n",
      "                                                                 \n",
      " conv1d_14 (Conv1D)          (None, 32, 128)           24704     \n",
      "                                                                 \n",
      " batch_normalization_33 (Ba  (None, 32, 128)           512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " global_average_pooling1d_4  (None, 128)               0         \n",
      "  (GlobalAveragePooling1D)                                       \n",
      "                                                                 \n",
      " dropout_33 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " batch_normalization_34 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_34 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 4)                 516       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 49476 (193.27 KB)\n",
      "Trainable params: 48772 (190.52 KB)\n",
      "Non-trainable params: 704 (2.75 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "69/69 [==============================] - 1s 11ms/step - loss: 1.6576 - accuracy: 0.3405 - val_loss: 1.1082 - val_accuracy: 0.5821\n",
      "Epoch 2/100\n",
      "69/69 [==============================] - 1s 11ms/step - loss: 1.3471 - accuracy: 0.4465 - val_loss: 1.0446 - val_accuracy: 0.5821\n",
      "Epoch 3/100\n",
      "69/69 [==============================] - 1s 11ms/step - loss: 1.2141 - accuracy: 0.5005 - val_loss: 1.1055 - val_accuracy: 0.5821\n",
      "Epoch 4/100\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 1.1300 - accuracy: 0.5338 - val_loss: 1.1327 - val_accuracy: 0.5821\n",
      "Epoch 5/100\n",
      "69/69 [==============================] - 1s 9ms/step - loss: 1.1017 - accuracy: 0.5270 - val_loss: 1.1210 - val_accuracy: 0.5821\n",
      "Epoch 6/100\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 1.0591 - accuracy: 0.5466 - val_loss: 1.0720 - val_accuracy: 0.5803\n",
      "Epoch 7/100\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 1.0407 - accuracy: 0.5526 - val_loss: 1.1398 - val_accuracy: 0.5839\n",
      "Epoch 8/100\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 1.0387 - accuracy: 0.5434 - val_loss: 1.0262 - val_accuracy: 0.5803\n",
      "Epoch 9/100\n",
      "69/69 [==============================] - 1s 11ms/step - loss: 1.0384 - accuracy: 0.5603 - val_loss: 1.0138 - val_accuracy: 0.5748\n",
      "Epoch 10/100\n",
      "69/69 [==============================] - 1s 11ms/step - loss: 1.0184 - accuracy: 0.5585 - val_loss: 0.9750 - val_accuracy: 0.5839\n",
      "Epoch 11/100\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 1.0078 - accuracy: 0.5663 - val_loss: 0.9965 - val_accuracy: 0.5785\n",
      "Epoch 12/100\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.9904 - accuracy: 0.5585 - val_loss: 0.9852 - val_accuracy: 0.5839\n",
      "Epoch 13/100\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.9829 - accuracy: 0.5663 - val_loss: 1.0201 - val_accuracy: 0.5821\n",
      "Epoch 14/100\n",
      "69/69 [==============================] - 1s 11ms/step - loss: 0.9682 - accuracy: 0.5754 - val_loss: 1.0262 - val_accuracy: 0.5803\n",
      "Epoch 15/100\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.9676 - accuracy: 0.5681 - val_loss: 0.9861 - val_accuracy: 0.5803\n",
      "Epoch 16/100\n",
      "69/69 [==============================] - 1s 11ms/step - loss: 0.9499 - accuracy: 0.5667 - val_loss: 0.9523 - val_accuracy: 0.5821\n",
      "Epoch 17/100\n",
      "69/69 [==============================] - 1s 11ms/step - loss: 0.9477 - accuracy: 0.5923 - val_loss: 0.9807 - val_accuracy: 0.5766\n",
      "Epoch 18/100\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.9493 - accuracy: 0.5841 - val_loss: 0.9509 - val_accuracy: 0.5785\n",
      "Epoch 19/100\n",
      "69/69 [==============================] - 1s 11ms/step - loss: 0.9571 - accuracy: 0.5704 - val_loss: 0.9620 - val_accuracy: 0.5821\n",
      "Epoch 20/100\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.9474 - accuracy: 0.5855 - val_loss: 0.9634 - val_accuracy: 0.5894\n",
      "Epoch 21/100\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.9404 - accuracy: 0.5891 - val_loss: 0.9902 - val_accuracy: 0.5858\n",
      "Epoch 22/100\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.9446 - accuracy: 0.5777 - val_loss: 0.9947 - val_accuracy: 0.5675\n",
      "Epoch 23/100\n",
      "69/69 [==============================] - 1s 11ms/step - loss: 0.9439 - accuracy: 0.5809 - val_loss: 0.9525 - val_accuracy: 0.5766\n",
      "Epoch 24/100\n",
      "69/69 [==============================] - 1s 11ms/step - loss: 0.9418 - accuracy: 0.5827 - val_loss: 0.9434 - val_accuracy: 0.5912\n",
      "Epoch 25/100\n",
      "69/69 [==============================] - 1s 11ms/step - loss: 0.9380 - accuracy: 0.5864 - val_loss: 0.9514 - val_accuracy: 0.5821\n",
      "Epoch 26/100\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.9231 - accuracy: 0.5791 - val_loss: 0.9678 - val_accuracy: 0.5785\n",
      "Epoch 27/100\n",
      "69/69 [==============================] - 1s 11ms/step - loss: 0.9350 - accuracy: 0.5882 - val_loss: 0.9401 - val_accuracy: 0.5876\n",
      "Epoch 28/100\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.9357 - accuracy: 0.5841 - val_loss: 0.9372 - val_accuracy: 0.5894\n",
      "Epoch 29/100\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.9161 - accuracy: 0.5951 - val_loss: 0.9595 - val_accuracy: 0.5766\n",
      "Epoch 30/100\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.9181 - accuracy: 0.5864 - val_loss: 1.0192 - val_accuracy: 0.5821\n",
      "Epoch 31/100\n",
      "69/69 [==============================] - 1s 11ms/step - loss: 0.9245 - accuracy: 0.5891 - val_loss: 0.9372 - val_accuracy: 0.5785\n",
      "Epoch 32/100\n",
      "69/69 [==============================] - 1s 11ms/step - loss: 0.9178 - accuracy: 0.5937 - val_loss: 0.9424 - val_accuracy: 0.5858\n",
      "Epoch 33/100\n",
      "69/69 [==============================] - 1s 11ms/step - loss: 0.9081 - accuracy: 0.5964 - val_loss: 0.9544 - val_accuracy: 0.5839\n",
      "Epoch 34/100\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.9076 - accuracy: 0.5960 - val_loss: 0.9682 - val_accuracy: 0.5803\n",
      "Epoch 35/100\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.9053 - accuracy: 0.5900 - val_loss: 0.9541 - val_accuracy: 0.5912\n",
      "Epoch 36/100\n",
      "69/69 [==============================] - 1s 11ms/step - loss: 0.9125 - accuracy: 0.5928 - val_loss: 0.9545 - val_accuracy: 0.5839\n",
      "Epoch 37/100\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.8924 - accuracy: 0.5910 - val_loss: 0.9688 - val_accuracy: 0.5657\n",
      "Epoch 38/100\n",
      "69/69 [==============================] - 1s 11ms/step - loss: 0.8963 - accuracy: 0.6037 - val_loss: 0.9564 - val_accuracy: 0.5894\n",
      "18/18 [==============================] - 0s 1ms/step\n",
      "cnn - Validation Loss: 0.9372, Validation Accuracy: 0.5894\n",
      "\n",
      "Meilleur modèle (basé sur la validation loss): linear\n"
     ]
    }
   ],
   "source": [
    "input_shape = (X_train.shape[1],)\n",
    "\n",
    "# Entraînement de tous les modèles\n",
    "models, histories, best_model_name, evaluations = train_all_models(\n",
    "    X_train, X_val, y_train, y_val, input_shape\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pour visualiser dans TensorBoard\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir logs/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_head = test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier de soumission généré : submission.csv\n",
      "\n",
      "Aperçu du fichier de soumission :\n",
      "         id  sii\n",
      "0  00008ff9    0\n",
      "1  000fd460    0\n",
      "2  00105258    0\n",
      "3  00115b9f    0\n",
      "4  0016bb22    0\n",
      "\n",
      "Distribution des prédictions :\n",
      "0    20\n",
      "Name: sii, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Génération du fichier de soumission avec le meilleur modèle\n",
    "best_model = models[best_model_name]\n",
    "submission = generate_submission(best_model, X_test, test_df)\n",
    "\n",
    "# Vérification du contenu du fichier de soumission\n",
    "print(\"\\nAperçu du fichier de soumission :\")\n",
    "print(submission.head())\n",
    "print(\"\\nDistribution des prédictions :\")\n",
    "print(submission['sii'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
